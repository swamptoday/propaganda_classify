{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import spacy\n",
    "import bz2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import xml.etree.ElementTree as ET\n",
    "import joblib\n",
    "\n",
    "from typing import *\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import openpyxl\n",
    "from pydantic import BaseModel\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ru-core-news-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/ru_core_news_sm-3.8.0/ru_core_news_sm-3.8.0-py3-none-any.whl (15.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.3/15.3 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pymorphy3>=1.0.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from ru-core-news-sm==3.8.0) (2.0.2)\n",
      "Requirement already satisfied: dawg-python>=0.7.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pymorphy3>=1.0.0->ru-core-news-sm==3.8.0) (0.7.2)\n",
      "Requirement already satisfied: pymorphy3-dicts-ru in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pymorphy3>=1.0.0->ru-core-news-sm==3.8.0) (2.4.417150.4580142)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('ru_core_news_sm')\n"
     ]
    }
   ],
   "source": [
    "!python3 -m spacy download ru_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/taraskozak/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nlp = spacy.load(\"uk_core_news_lg\")\n",
    "nlp_2 = spacy.load(\"ru_core_news_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = joblib.load('../server/scaler.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "Some weights of the model checkpoint at DeepPavlov/rubert-base-cased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model_2 = load_model(\"../server/model_2.h5\")\n",
    "\n",
    "# Load the pre-trained tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"DeepPavlov/rubert-base-cased\")\n",
    "bert_model = AutoModel.from_pretrained(\"DeepPavlov/rubert-base-cased\")\n",
    "\n",
    "# Define unique labels\n",
    "unique_labels = ['Заперечення', 'Виправдовування', 'Заклик', \n",
    "                 'Розпалювання ворожнечі та ненависті', \n",
    "                 'Приниження національної честі та гідності', \n",
    "                 'Просто текст']\n",
    "\n",
    "label_to_id = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "id_to_label = {idx: label for label, idx in label_to_id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    nlp = spacy.load(\"ru_core_news_sm\")  # Adjust to your language model\n",
    "    doc = nlp(str(text).lower()) \n",
    "    tokens = [token.lemma_ for token in doc if not token.is_stop and token.is_alpha]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "def get_avg_w2v(text, tokenizer, bert_model):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = bert_model(**inputs)\n",
    "    vector = outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_named_entities(text):\n",
    "    doc = nlp(text)\n",
    "    location_count = sum(1 for ent in doc.ents if ent.label_ == \"LOC\")  # Count locations\n",
    "    organization_count = sum(1 for ent in doc.ents if ent.label_ == \"ORG\")  # Count organizations\n",
    "    return location_count, organization_count\n",
    "def avg_noun_verb_ratio(text):\n",
    "    doc = nlp(text)\n",
    "    ratios = []\n",
    "    \n",
    "    for sent in doc.sents:  # Process each sentence separately\n",
    "        nouns = sum(1 for token in sent if token.pos_ == \"NOUN\")\n",
    "        verbs = sum(1 for token in sent if token.pos_ == \"VERB\")\n",
    "        if verbs > 0:\n",
    "            ratios.append(nouns / verbs)  # Compute noun/verb ratio\n",
    "        else:\n",
    "            ratios.append(0)  # Avoid division by zero\n",
    "\n",
    "    return sum(ratios) / len(ratios) if ratios else 0  # Compute the average\n",
    "def calculate_subj(text):\n",
    "    \n",
    "    subj_dict_synt = {}\n",
    "\n",
    "    tree = ET.parse(\"../server/translated_output.xml\")\n",
    "    root = tree.getroot()\n",
    "\n",
    "    # Extract words and polarity\n",
    "    for word in root.findall(\"word\"):\n",
    "        word_form = word.get(\"form\")\n",
    "        polarity = float(word.get(\"subjectivity\", 0))  # Default polarity = 0 if not present\n",
    "        subj_dict_synt[word_form] = polarity\n",
    "    \n",
    "    if not isinstance(text, str):\n",
    "        return 0.0  # Return neutral score for missing values\n",
    "    words = text.split()  # Tokenize text\n",
    "    score = sum(subj_dict_synt.get(word, 0) for word in words)  # Sum word polarities\n",
    "    return score\n",
    "def calculate_sentiment(text):\n",
    "    \n",
    "    sentiment_dict_synt = {}\n",
    "\n",
    "    tree = ET.parse(\"../server/translated_output.xml\")\n",
    "    root = tree.getroot()\n",
    "\n",
    "    # Extract words and polarity\n",
    "    for word in root.findall(\"word\"):\n",
    "        word_form = word.get(\"form\")\n",
    "        polarity = float(word.get(\"polarity\", 0))  # Default polarity = 0 if not present\n",
    "        sentiment_dict_synt[word_form] = polarity\n",
    "    \n",
    "    if not isinstance(text, str):\n",
    "        return 0.0  # Return neutral score for missing values\n",
    "    words = text.split()  # Tokenize text\n",
    "    score = sum(sentiment_dict_synt.get(word, 0) for word in words)  # Sum word polarities\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text: str):\n",
    "    \n",
    "    # Load your SpaCy model\n",
    "    nlp = spacy.load('uk_core_news_lg')  # Or the model you're using for lemmatization\n",
    "\n",
    "    # Load GloVe word vectors\n",
    "    glove_path = \"../server//news.lowercased.lemmatized.glove.300d.bz2\"\n",
    "    word_vectors = {}\n",
    "\n",
    "    # Load GloVe vectors into memory\n",
    "    with bz2.open(glove_path, \"rt\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split()\n",
    "            word = parts[0]\n",
    "            vector = np.array(parts[1:], dtype=np.float32)\n",
    "            word_vectors[word] = vector\n",
    "            \n",
    "    # --- Step 1: Text Preprocessing ---\n",
    "    \n",
    "    # 1.1: Tokenize and lemmatize using Spacy\n",
    "    doc = nlp(text)\n",
    "    lemmatized_text = ' '.join([token.lemma_ for token in doc])\n",
    "    \n",
    "    # 1.2: Extract features like punctuation\n",
    "    has_colons = 1 if ':' in text else 0\n",
    "    has_hyphens = 1 if '-' in text else 0\n",
    "    has_quotmarks = 1 if '\"' in text else 0\n",
    "    \n",
    "    # 1.3: Sentiment & subjectivity (assuming functions are available for sentiment analysis)\n",
    "    sentiment = calculate_sentiment(text)\n",
    "    subjectiveness = calculate_subj(text)\n",
    "    \n",
    "    # 1.4: Noun-verb ratio calculation (assuming it's already done)\n",
    "    noun_verb_ratio = avg_noun_verb_ratio(text)\n",
    "    \n",
    "    # 1.5: Count location and organization mentions (assuming functions are available)\n",
    "    location_count, organization_count = count_named_entities(text)\n",
    "    \n",
    "    # --- Step 2: Vectorize the text (using GloVe) ---\n",
    "    vectors = []\n",
    "    for token in doc:\n",
    "        word = token.lemma_.lower()\n",
    "        if word in word_vectors:\n",
    "            vectors.append(word_vectors[word])\n",
    "    \n",
    "    # If there are no valid words found in the GloVe vocabulary, return a zero vector\n",
    "    if not vectors:\n",
    "        vector = np.zeros(300)  # 300 is the dimension of GloVe vectors\n",
    "    else:\n",
    "        vector = np.mean(vectors, axis=0)\n",
    "    \n",
    "    # --- Step 3: Combine all features into a single vector ---\n",
    "    scaled_features = scaler.transform([[sentiment, subjectiveness, noun_verb_ratio, location_count, organization_count]])[0]\n",
    "\n",
    "    # Combine all features\n",
    "    features = np.concatenate([\n",
    "        vector,  # GloVe vector (300D)\n",
    "        [has_colons, has_hyphens, has_quotmarks],  # Binary features\n",
    "        scaled_features  # Standardized numeric features\n",
    "    ])\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_sentence(text):\n",
    "    # Preprocess the input sentence\n",
    "    processed_text = preprocess_text(text)\n",
    "    \n",
    "    # Convert the processed text to a vector (наприклад, Word2Vec або будь-який інший)\n",
    "    vector = get_avg_w2v(processed_text, tokenizer, bert_model)\n",
    "    \n",
    "    vector = vector[:767]\n",
    "\n",
    "    # Передбачення ймовірностей\n",
    "    probabilities = model_2.predict(np.array([vector]))\n",
    "\n",
    "    # Нормалізовані ймовірності (якщо необхідно)\n",
    "    probabilities_dict = {\n",
    "        id_to_label[i]: float(probabilities[0][i]) for i in range(len(probabilities[0]))\n",
    "    }\n",
    "\n",
    "    # Найвірогідніший клас\n",
    "    predicted_label = id_to_label[np.argmax(probabilities)]\n",
    "\n",
    "    return {\n",
    "        \"text\": text,\n",
    "        \"processed_text\": processed_text,\n",
    "        \"label\": predicted_label,\n",
    "        \"probabilities\": probabilities_dict,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/keras/src/saving/saving_lib.py:757: UserWarning: Skipping variable loading for optimizer 'rmsprop', because it has 10 variables whereas the saved optimizer has 18 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    }
   ],
   "source": [
    "model = load_model(\"../server/model.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/base.py:493: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Example text\n",
    "text = \"Хочете допомогу Києву, платіть колоніальні репарації: влада Євросоюзу зробила велику помилку на переговорах з Латинською Америкою.\"\n",
    "\n",
    "# Preprocess the text and get the feature vector\n",
    "features = preprocess_text(text)\n",
    "\n",
    "# Now you can use this feature vector as input to your trained model\n",
    "prediction = model.predict(np.expand_dims(features, axis=0))  # Add batch dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1]]\n"
     ]
    }
   ],
   "source": [
    "predicted_label = (prediction >= 0.5).astype(int)\n",
    "print(predicted_label)  # Output: [[1]]\n",
    "\n",
    "if(predicted_label == [[1]]):\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
